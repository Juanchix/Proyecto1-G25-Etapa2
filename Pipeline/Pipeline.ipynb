{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f7bbd544-da1c-4a85-be81-6b620b88e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# librería Natural Language Toolkit, usada para trabajar con textos\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# librería para manejar las flexiones gramaticales en el idioma español.\n",
    "import spacy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string, unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mode\n",
    "from scipy import stats as st\n",
    "from num2words import num2words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "more_stopwords = ['ser','estar', 'tener', 'haber']\n",
    "stop_words = stop_words.union(more_stopwords)   \n",
    "# Load Spanish language model\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd2c143e-4df4-45cc-acd1-26e9b0a4820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la libreria pandas para la lectura de archivos\n",
    "data = pd.read_csv('tipo1_entrenamiento_estudiantes.csv', sep=',', encoding = 'utf-8')\n",
    "textos = data.copy()\n",
    "datos_limpios = data.copy()\n",
    "datos_limpios.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec5614b4-2909-4330-950a-de21d2846645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_duplicados(Pdataframe, Psubset):\n",
    "    Pdataframe = Pdataframe.drop_duplicates(subset=Psubset)\n",
    "    Pdataframe.reset_index(drop=True, inplace=True)\n",
    "    return Pdataframe\n",
    "\n",
    "def convertir_minusculas(texto):\n",
    "    \"\"\"Convierte un string a minúsculas\"\"\"\n",
    "    return texto.lower()\n",
    "\n",
    "def convertir_enteros_a_texto(texto):\n",
    "    \"\"\"Convierte los números enteros en su versión textual en español en un texto dado.\"\"\"\n",
    "    def reemplazar(match):\n",
    "        numero = int(match.group(0))\n",
    "        return num2words(numero, lang='es')\n",
    "    texto_convertido = re.sub(r'\\b\\d+\\b', reemplazar, texto)\n",
    "    return texto_convertido\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters while preserving the structure of the text\"\"\"\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if ord(char) < 128:  # Si el carácter es ASCII\n",
    "            cleaned_text += char  # Conserva el carácter\n",
    "        else:\n",
    "            cleaned_text += unicodedata.normalize('NFKD', char).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Reemplaza el carácter no ASCII con su equivalente ASCII ignorando los no convertibles\n",
    "    return cleaned_text\n",
    "\n",
    "def eliminar_caracteres_especiales(texto):\n",
    "    \"\"\"Elimina los caracteres especiales como \\r, \\n, etc., de un texto.\"\"\"\n",
    "    texto_limpio = re.sub(r'[\\r\\n\\t]', ' ', texto)\n",
    "    # La expresión regular r'[\\r\\n\\t]' coincide con \\r, \\n y \\t\n",
    "    return texto_limpio\n",
    "\n",
    "\n",
    "def lemmatize_spanish_text_batch(datos_limpios, column_name=\"Review\", batch_size=500):\n",
    "    def lemmatize_spanish_text(text):\n",
    "        # Process the text\n",
    "        doc = nlp(text)\n",
    "        # Lemmatize each token and return the lemmatized text\n",
    "        lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "        return lemmatized_text\n",
    "\n",
    "    # Batch processing with DataFrame.apply\n",
    "    num_batches = (len(datos_limpios) - 1) // batch_size + 1  # Adjusted to ensure all rows are processed\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(datos_limpios))\n",
    "        datos_limpios.loc[start_idx:end_idx, column_name] = datos_limpios.loc[start_idx:end_idx, column_name].apply(lemmatize_spanish_text)\n",
    "        print(f\"Processed batch {i+1}/{num_batches}\")\n",
    "\n",
    "    # Process the remaining rows if any\n",
    "    start_idx = num_batches * batch_size\n",
    "    if start_idx < len(datos_limpios):\n",
    "        datos_limpios.loc[start_idx:, column_name] = datos_limpios.loc[start_idx:, column_name].apply(lemmatize_spanish_text)\n",
    "    \n",
    "    return datos_limpios\n",
    "\n",
    "\n",
    "def eliminar_tildes_y_puntuacion(texto):\n",
    "    # Eliminar tildes y acentos\n",
    "    texto_sin_tildes = unidecode(texto)\n",
    "    # Eliminar signos de puntuación\n",
    "    texto_sin_puntuacion = texto_sin_tildes.translate(str.maketrans('', '', string.punctuation))\n",
    "    return texto_sin_puntuacion\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenización de la reseña\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  # Filtrado de stopwords\n",
    "    return ' '.join(filtered_tokens)  # Reconstruir la reseña sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db3550fb-70ae-45df-bff2-c285f55e5d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 50.413 %\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.40      0.48       160\n",
      "           2       0.53      0.42      0.47       245\n",
      "           3       0.41      0.43      0.42       288\n",
      "           4       0.43      0.41      0.42       409\n",
      "           5       0.58      0.70      0.64       473\n",
      "\n",
      "    accuracy                           0.50      1575\n",
      "   macro avg       0.51      0.47      0.49      1575\n",
      "weighted avg       0.50      0.50      0.50      1575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_2 = pd.read_csv(\"particion_prueba_estudiantes.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "datos_limpios_2 = data_2.copy()\n",
    "\n",
    "# Supongamos que datos_limpios es un DataFrame con las columnas Review y Class\n",
    "data_set = datos_limpios.copy()\n",
    "data_test = datos_limpios_2.copy()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "x = int(len(data_set) * 0.8)\n",
    "train, test = data_set.iloc[:x, :], data_set.iloc[x:, :]\n",
    "\n",
    "# Separar las características (Review) y las etiquetas (Class)\n",
    "X_train, y_train = train['Review'], train['Class']\n",
    "X_test, y_test = test['Review'], test['Class']\n",
    "\n",
    "# Convertir texto a características numéricas utilizando TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=3500)  # Utilizamos solo las 3500 características más importantes\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "X_data_test = vectorizer.transform(data_test[\"Review\"])\n",
    "\n",
    "# Entrenar un modelo de regresión logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predecir las etiquetas para el conjunto de prueba\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "class_prediction = model.predict(X_data_test)\n",
    "data_test[\"Class\"] = class_prediction\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo:\", round(accuracy, 5)*100, \"%\")\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "data_test.to_csv(\"particion_prueba_estudiantes_predicted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4b04f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_limpios.to_csv(\"Datos_limpios.csv\", index=False)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"duplicates\", eliminar_duplicados),\n",
    "    (\"minusculas\", convertir_minusculas),\n",
    "    (\"numeros enteros\", convertir_enteros_a_texto),\n",
    "    (\"ascii\", remove_non_ascii),\n",
    "    (\"caracteres_especiales\", eliminar_caracteres_especiales),\n",
    "    (\"puntuacion\", eliminar_tildes_y_puntuacion),\n",
    "    (\"stopwords\", remove_stopwords)\n",
    "])\n",
    "\n",
    "datos_limpios_2 = eliminar_duplicados(datos_limpios_2, [\"Review\"])\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(convertir_minusculas)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(convertir_enteros_a_texto)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(remove_non_ascii)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(eliminar_caracteres_especiales)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(eliminar_tildes_y_puntuacion)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(remove_stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
