{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7bbd544-da1c-4a85-be81-6b620b88e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# librería Natural Language Toolkit, usada para trabajar con textos\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# librería para manejar las flexiones gramaticales en el idioma español.\n",
    "import spacy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string, unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mode\n",
    "from scipy import stats as st\n",
    "from num2words import num2words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "# Load Spanish language model\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd2c143e-4df4-45cc-acd1-26e9b0a4820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la libreria pandas para la lectura de archivos\n",
    "data = pd.read_csv('tipo1_entrenamiento_estudiantes.csv', sep=',', encoding = 'utf-8')\n",
    "textos = data.copy()\n",
    "datos_limpios = data.copy()\n",
    "datos_limpios.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec5614b4-2909-4330-950a-de21d2846645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_duplicados(Pdataframe, Psubset):\n",
    "    Pdataframe = Pdataframe.drop_duplicates(subset=Psubset)\n",
    "    Pdataframe.reset_index(drop=True, inplace=True)\n",
    "    return Pdataframe\n",
    "\n",
    "def convertir_minusculas(texto):\n",
    "    \"\"\"Convierte un string a minúsculas\"\"\"\n",
    "    return texto.lower()\n",
    "\n",
    "def convertir_enteros_a_texto(texto):\n",
    "    \"\"\"Convierte los números enteros en su versión textual en español en un texto dado.\"\"\"\n",
    "    def reemplazar(match):\n",
    "        numero = int(match.group(0))\n",
    "        return num2words(numero, lang='es')\n",
    "\n",
    "    texto_convertido = re.sub(r'\\b\\d+\\b', reemplazar, texto)\n",
    "    return texto_convertido\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters while preserving the structure of the text\"\"\"\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if ord(char) < 128:  # Si el carácter es ASCII\n",
    "            cleaned_text += char  # Conserva el carácter\n",
    "        else:\n",
    "            cleaned_text += unicodedata.normalize('NFKD', char).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Reemplaza el carácter no ASCII con su equivalente ASCII ignorando los no convertibles\n",
    "    return cleaned_text\n",
    "\n",
    "def eliminar_caracteres_especiales(texto):\n",
    "    \"\"\"Elimina los caracteres especiales como \\r, \\n, etc., de un texto.\"\"\"\n",
    "    texto_limpio = re.sub(r'[\\r\\n\\t]', ' ', texto)\n",
    "    # La expresión regular r'[\\r\\n\\t]' coincide con \\r, \\n y \\t\n",
    "    return texto_limpio\n",
    "\n",
    "\n",
    "def lemmatize_spanish_text_batch(datos_limpios, column_name=\"Review\", batch_size=500):\n",
    "    def lemmatize_spanish_text(text):\n",
    "        # Process the text\n",
    "        doc = nlp(text)\n",
    "        # Lemmatize each token and return the lemmatized text\n",
    "        lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "        return lemmatized_text\n",
    "\n",
    "    # Batch processing with DataFrame.apply\n",
    "    num_batches = (len(datos_limpios) - 1) // batch_size + 1  # Adjusted to ensure all rows are processed\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(datos_limpios))\n",
    "        datos_limpios.loc[start_idx:end_idx, column_name] = datos_limpios.loc[start_idx:end_idx, column_name].apply(lemmatize_spanish_text)\n",
    "        print(f\"Processed batch {i+1}/{num_batches}\")\n",
    "\n",
    "    # Process the remaining rows if any\n",
    "    start_idx = num_batches * batch_size\n",
    "    if start_idx < len(datos_limpios):\n",
    "        datos_limpios.loc[start_idx:, column_name] = datos_limpios.loc[start_idx:, column_name].apply(lemmatize_spanish_text)\n",
    "    \n",
    "    return datos_limpios\n",
    "\n",
    "\n",
    "def eliminar_tildes_y_puntuacion(texto):\n",
    "    # Eliminar tildes y acentos\n",
    "    texto_sin_tildes = unidecode(texto)\n",
    "    \n",
    "    # Eliminar signos de puntuación\n",
    "    texto_sin_puntuacion = texto_sin_tildes.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return texto_sin_puntuacion\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    more_stopwords = ['ser','estar', 'tener', 'haber']\n",
    "    stop_words = stop_words.union(more_stopwords)   \n",
    "    tokens = nltk.word_tokenize(text)  # Tokenización de la reseña\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  # Filtrado de stopwords\n",
    "    return ' '.join(filtered_tokens)  # Reconstruir la reseña sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db3550fb-70ae-45df-bff2-c285f55e5d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 50.413 %\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.40      0.48       160\n",
      "           2       0.53      0.42      0.47       245\n",
      "           3       0.41      0.43      0.42       288\n",
      "           4       0.43      0.41      0.42       409\n",
      "           5       0.58      0.70      0.64       473\n",
      "\n",
      "    accuracy                           0.50      1575\n",
      "   macro avg       0.51      0.47      0.49      1575\n",
      "weighted avg       0.50      0.50      0.50      1575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_2 = pd.read_csv(\"particion_prueba_estudiantes.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "datos_limpios_2 = data_2.copy()\n",
    "\n",
    "# Supongamos que datos_limpios es un DataFrame con las columnas Review y Class\n",
    "data_set = datos_limpios.copy()\n",
    "data_test = datos_limpios_2.copy()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "x = int(len(data_set) * 0.8)\n",
    "train, test = data_set.iloc[:x, :], data_set.iloc[x:, :]\n",
    "\n",
    "# Separar las características (Review) y las etiquetas (Class)\n",
    "X_train, y_train = train['Review'], train['Class']\n",
    "X_test, y_test = test['Review'], test['Class']\n",
    "\n",
    "# Convertir texto a características numéricas utilizando TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=3500)  # Utilizamos solo las 3500 características más importantes\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "X_data_test = vectorizer.transform(data_test[\"Review\"])\n",
    "\n",
    "# Entrenar un modelo de regresión logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predecir las etiquetas para el conjunto de prueba\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "class_prediction = model.predict(X_data_test)\n",
    "data_test[\"Class\"] = class_prediction\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo:\", round(accuracy, 5)*100, \"%\")\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "data_test.to_csv(\"particion_prueba_estudiantes_predicted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b04f64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'stop_words' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m datos_limpios_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m datos_limpios_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(eliminar_caracteres_especiales)\n\u001b[0;32m     18\u001b[0m datos_limpios_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m datos_limpios_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(eliminar_tildes_y_puntuacion)\n\u001b[1;32m---> 19\u001b[0m datos_limpios_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdatos_limpios_2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4780\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[61], line 75\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(text):\n\u001b[0;32m     74\u001b[0m     more_stopwords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mser\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtener\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaber\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 75\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[43mstop_words\u001b[49m\u001b[38;5;241m.\u001b[39munion(more_stopwords)   \n\u001b[0;32m     76\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)  \u001b[38;5;66;03m# Tokenización de la reseña\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Filtrado de stopwords\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'stop_words' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "datos_limpios.to_csv(\"Datos_limpios.csv\", index=False)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"duplicates\", eliminar_duplicados),\n",
    "    (\"minusculas\", convertir_minusculas),\n",
    "    (\"numeros enteros\", convertir_enteros_a_texto),\n",
    "    (\"ascii\", remove_non_ascii),\n",
    "    (\"caracteres_especiales\", eliminar_caracteres_especiales),\n",
    "    (\"puntuacion\", eliminar_tildes_y_puntuacion),\n",
    "    (\"stopwords\", remove_stopwords)\n",
    "])\n",
    "\n",
    "datos_limpios_2 = eliminar_duplicados(datos_limpios_2, [\"Review\"])\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(convertir_minusculas)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(convertir_enteros_a_texto)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(remove_non_ascii)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(eliminar_caracteres_especiales)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(eliminar_tildes_y_puntuacion)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(remove_stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
