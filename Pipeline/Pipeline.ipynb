{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7bbd544-da1c-4a85-be81-6b620b88e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# librería Natural Language Toolkit, usada para trabajar con textos\n",
    "import nltk\n",
    "\n",
    "# librería para manejar las flexiones gramaticales en el idioma español.\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer ###\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "\n",
    "# Punkt permite separar un texto en frases.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "from statistics import mode\n",
    "from scipy import stats as st\n",
    "\n",
    "\n",
    "import ydata_profiling as dp\n",
    "from num2words import num2words\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd2c143e-4df4-45cc-acd1-26e9b0a4820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la libreria pandas para la lectura de archivos\n",
    "data = pd.read_csv('tipo1_entrenamiento_estudiantes.csv', sep=',', encoding = 'utf-8')\n",
    "textos = data.copy()\n",
    "datos_limpios = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec5614b4-2909-4330-950a-de21d2846645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/16\n",
      "Processed batch 2/16\n",
      "Processed batch 3/16\n",
      "Processed batch 4/16\n",
      "Processed batch 5/16\n",
      "Processed batch 6/16\n",
      "Processed batch 7/16\n",
      "Processed batch 8/16\n",
      "Processed batch 9/16\n",
      "Processed batch 10/16\n",
      "Processed batch 11/16\n",
      "Processed batch 12/16\n",
      "Processed batch 13/16\n",
      "Processed batch 14/16\n",
      "Processed batch 15/16\n",
      "Processed batch 16/16\n"
     ]
    }
   ],
   "source": [
    "def eliminar_duplicados(Pdataframe, Psubset):\n",
    "    Pdataframe = Pdataframe.drop_duplicates(subset=Psubset)\n",
    "    Pdataframe.reset_index(drop=True, inplace=True)\n",
    "    return Pdataframe\n",
    "\n",
    "def convertir_minusculas(texto):\n",
    "    \"\"\"Convierte un string a minúsculas\"\"\"\n",
    "    return texto.lower()\n",
    "\n",
    "def convertir_enteros_a_texto(texto):\n",
    "    \"\"\"Convierte los números enteros en su versión textual en español en un texto dado.\"\"\"\n",
    "    def reemplazar(match):\n",
    "        numero = int(match.group(0))\n",
    "        return num2words(numero, lang='es')\n",
    "\n",
    "    texto_convertido = re.sub(r'\\b\\d+\\b', reemplazar, texto)\n",
    "    return texto_convertido\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters while preserving the structure of the text\"\"\"\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if ord(char) < 128:  # Si el carácter es ASCII\n",
    "            cleaned_text += char  # Conserva el carácter\n",
    "        else:\n",
    "            cleaned_text += unicodedata.normalize('NFKD', char).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Reemplaza el carácter no ASCII con su equivalente ASCII ignorando los no convertibles\n",
    "    return cleaned_text\n",
    "\n",
    "def eliminar_caracteres_especiales(texto):\n",
    "    \"\"\"Elimina los caracteres especiales como \\r, \\n, etc., de un texto.\"\"\"\n",
    "    texto_limpio = re.sub(r'[\\r\\n\\t]', ' ', texto)\n",
    "    # La expresión regular r'[\\r\\n\\t]' coincide con \\r, \\n y \\t\n",
    "    return texto_limpio\n",
    "\n",
    "\n",
    "\n",
    "datos_limpios.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Load Spanish language model\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "def lemmatize_spanish_text(text):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Lemmatize each token and return the lemmatized text\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "\n",
    "# Batch processing with DataFrame.apply\n",
    "batch_size = 500\n",
    "num_batches = (len(datos_limpios) - 1) // batch_size + 1  # Adjusted to ensure all rows are processed\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(datos_limpios))\n",
    "    datos_limpios.loc[start_idx:end_idx, \"Review\"] = datos_limpios.loc[start_idx:end_idx, \"Review\"].apply(lemmatize_spanish_text)\n",
    "    print(f\"Processed batch {i+1}/{num_batches}\")\n",
    "\n",
    "# Process the remaining rows if any\n",
    "start_idx = num_batches * batch_size\n",
    "if start_idx < len(datos_limpios):\n",
    "    datos_limpios.loc[start_idx:, \"Review\"] = datos_limpios.loc[start_idx:, \"Review\"].apply(lemmatize_spanish_text)\n",
    "\n",
    "\n",
    "def eliminar_tildes_y_puntuacion(texto):\n",
    "    # Eliminar tildes y acentos\n",
    "    texto_sin_tildes = unidecode(texto)\n",
    "    \n",
    "    # Eliminar signos de puntuación\n",
    "    texto_sin_puntuacion = texto_sin_tildes.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return texto_sin_puntuacion\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "more_stopwords = ['ser','estar', 'tener', 'haber']\n",
    "stop_words = stop_words.union(more_stopwords)\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenización de la reseña\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  # Filtrado de stopwords\n",
    "    return ' '.join(filtered_tokens)  # Reconstruir la reseña sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "049b7de0-ec83-4d10-8617-224afe08fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_limpios.to_csv(\"Datos_limpios.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2185de2-4359-486c-8f79-c1348fc8ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_limpios.to_csv(\"Datos_limpios.csv\", index=False)\n",
    "\n",
    "data_2 = pd.read_csv(\"particion_prueba_estudiantes.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "pipeline = Pipeline([\n",
    "    (\"duplicates\", eliminar_duplicados),\n",
    "    (\"minusculas\", convertir_minusculas),\n",
    "    (\"numeros enteros\", convertir_enteros_a_texto),\n",
    "    (\"ascii\", remove_non_ascii),\n",
    "    (\"caracteres_especiales\", eliminar_caracteres_especiales),\n",
    "    (\"puntuacion\", eliminar_tildes_y_puntuacion),\n",
    "    (\"stopwords\", remove_stopwords)\n",
    "])\n",
    "datos_limpios_2 = data_2.copy()\n",
    "datos_limpios_2 = eliminar_duplicados(datos_limpios_2, [\"Review\"])\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(convertir_minusculas)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(convertir_enteros_a_texto)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(remove_non_ascii)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(eliminar_caracteres_especiales)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(eliminar_tildes_y_puntuacion)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db3550fb-70ae-45df-bff2-c285f55e5d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 50.54 %\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.37      0.43       160\n",
      "           2       0.47      0.38      0.42       245\n",
      "           3       0.39      0.42      0.41       288\n",
      "           4       0.45      0.44      0.45       409\n",
      "           5       0.61      0.72      0.66       473\n",
      "\n",
      "    accuracy                           0.51      1575\n",
      "   macro avg       0.49      0.47      0.47      1575\n",
      "weighted avg       0.50      0.51      0.50      1575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Supongamos que datos_limpios es un DataFrame con las columnas Review y Class\n",
    "data_set = datos_limpios.copy()\n",
    "data_test = datos_limpios_2.copy()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "x = int(len(data_set) * 0.8)\n",
    "train, test = data_set.iloc[:x, :], data_set.iloc[x:, :]\n",
    "\n",
    "# Separar las características (Review) y las etiquetas (Class)\n",
    "X_train, y_train = train['Review'], train['Class']\n",
    "X_test, y_test = test['Review'], test['Class']\n",
    "\n",
    "# Convertir texto a características numéricas utilizando TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=2500)  # Utilizamos solo las 1000 características más importantes\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "X_data_test = vectorizer.transform(data_test[\"Review\"])\n",
    "\n",
    "# Entrenar un modelo de regresión logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predecir las etiquetas para el conjunto de prueba\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "class_prediction = model.predict(X_data_test)\n",
    "data_test[\"Class\"] = class_prediction\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo:\", round(accuracy, 5)*100, \"%\")\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "data_test.to_csv(\"particion_prueba_estudiantes_predicted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa588b8-48bb-4430-a4f3-d0ba04e3322f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
