{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7bbd544-da1c-4a85-be81-6b620b88e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\orteg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# librería Natural Language Toolkit, usada para trabajar con textos\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# librería para manejar las flexiones gramaticales en el idioma español.\n",
    "import spacy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string, unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mode\n",
    "from scipy import stats as st\n",
    "from num2words import num2words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "more_stopwords = ['ser','estar', 'tener', 'haber']\n",
    "stop_words = stop_words.union(more_stopwords)   \n",
    "# Load Spanish language model\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dd2c143e-4df4-45cc-acd1-26e9b0a4820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la libreria pandas para la lectura de archivos\n",
    "data = pd.read_csv('tipo1_entrenamiento_estudiantes.csv', sep=',', encoding = 'utf-8')\n",
    "textos = data.copy()\n",
    "datos_limpios = data.copy()\n",
    "datos_limpios.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec5614b4-2909-4330-950a-de21d2846645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_duplicados(Pdataframe, Psubset):\n",
    "    Pdataframe = Pdataframe.drop_duplicates(subset=Psubset)\n",
    "    Pdataframe.reset_index(drop=True, inplace=True)\n",
    "    return Pdataframe\n",
    "\n",
    "def convertir_minusculas(texto):\n",
    "    \"\"\"Convierte un string a minúsculas\"\"\"\n",
    "    return texto.lower()\n",
    "\n",
    "def convertir_enteros_a_texto(texto):\n",
    "    \"\"\"Convierte los números enteros en su versión textual en español en un texto dado.\"\"\"\n",
    "    def reemplazar(match):\n",
    "        numero = int(match.group(0))\n",
    "        return num2words(numero, lang='es')\n",
    "    texto_convertido = re.sub(r'\\b\\d+\\b', reemplazar, texto)\n",
    "    return texto_convertido\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters while preserving the structure of the text\"\"\"\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if ord(char) < 128:  # Si el carácter es ASCII\n",
    "            cleaned_text += char  # Conserva el carácter\n",
    "        else:\n",
    "            cleaned_text += unicodedata.normalize('NFKD', char).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Reemplaza el carácter no ASCII con su equivalente ASCII ignorando los no convertibles\n",
    "    return cleaned_text\n",
    "\n",
    "def eliminar_caracteres_especiales(texto):\n",
    "    \"\"\"Elimina los caracteres especiales como \\r, \\n, etc., de un texto.\"\"\"\n",
    "    texto_limpio = re.sub(r'[\\r\\n\\t]', ' ', texto)\n",
    "    # La expresión regular r'[\\r\\n\\t]' coincide con \\r, \\n y \\t\n",
    "    return texto_limpio\n",
    "\n",
    "\n",
    "def lemmatize_spanish_text_batch(datos_limpios, column_name=\"Review\", batch_size=500):\n",
    "    def lemmatize_spanish_text(text):\n",
    "        # Process the text\n",
    "        doc = nlp(text)\n",
    "        # Lemmatize each token and return the lemmatized text\n",
    "        lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "        return lemmatized_text\n",
    "\n",
    "    # Batch processing with DataFrame.apply\n",
    "    num_batches = (len(datos_limpios) - 1) // batch_size + 1  # Adjusted to ensure all rows are processed\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(datos_limpios))\n",
    "        datos_limpios.loc[start_idx:end_idx, column_name] = datos_limpios.loc[start_idx:end_idx, column_name].apply(lemmatize_spanish_text)\n",
    "        print(f\"Processed batch {i+1}/{num_batches}\")\n",
    "\n",
    "    # Process the remaining rows if any\n",
    "    start_idx = num_batches * batch_size\n",
    "    if start_idx < len(datos_limpios):\n",
    "        datos_limpios.loc[start_idx:, column_name] = datos_limpios.loc[start_idx:, column_name].apply(lemmatize_spanish_text)\n",
    "    \n",
    "    return datos_limpios\n",
    "\n",
    "\n",
    "def eliminar_tildes_y_puntuacion(texto):\n",
    "    # Eliminar tildes y acentos\n",
    "    texto_sin_tildes = unidecode(texto)\n",
    "    # Eliminar signos de puntuación\n",
    "    texto_sin_puntuacion = texto_sin_tildes.translate(str.maketrans('', '', string.punctuation))\n",
    "    return texto_sin_puntuacion\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenización de la reseña\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  # Filtrado de stopwords\n",
    "    return ' '.join(filtered_tokens)  # Reconstruir la reseña sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db3550fb-70ae-45df-bff2-c285f55e5d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "def process_and_predict(data_train_path, data_test_path):\n",
    "\n",
    "    data_2 = pd.read_csv(\"particion_prueba_estudiantes.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "    datos_limpios_2 = data_2.copy()\n",
    "    # Leer los datos\n",
    "    data_set = datos_limpios.copy()\n",
    "    data_test = datos_limpios_2.copy()\n",
    "\n",
    "    # Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "    x = int(len(data_set) * 0.8)\n",
    "    train, test = data_set.iloc[:x, :], data_set.iloc[x:, :]\n",
    "\n",
    "    # Separar las características (Review) y las etiquetas (Class)\n",
    "    x_train, y_train = train['Review'], train['Class']\n",
    "    x_test, y_test = test['Review'], test['Class']\n",
    "\n",
    "    # Convertir texto a características numéricas utilizando TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=3500)  # Utilizamos solo las 3500 características más importantes\n",
    "    x_train_vec = vectorizer.fit_transform(x_train)\n",
    "    x_test_vec = vectorizer.transform(x_test)\n",
    "    x_data_test = vectorizer.transform(data_test[\"Review\"])\n",
    "\n",
    "    # Entrenar un modelo de regresión logística\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x_train_vec, y_train)\n",
    "\n",
    "    # Predecir las etiquetas para el conjunto de prueba\n",
    "    y_pred = model.predict(x_test_vec)\n",
    "\n",
    "    class_prediction = model.predict(x_data_test)\n",
    "    data_test[\"Class\"] = class_prediction\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Precisión del modelo:\", round(accuracy, 5)*100, \"%\")\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Guardar las predicciones\n",
    "    data_test.to_csv(\"particion_prueba_estudiantes_predicted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b04f64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<function convertir_minusculas at 0x000002662707F1A0>' (type <class 'function'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Entrena el pipeline\u001b[39;00m\n\u001b[0;32m     17\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m datos_limpios[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m], datos_limpios[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 18\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(pipeline, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m loaded_pipeline \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:388\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, routed_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;66;03m# shallow copy of steps - this should really be steps_\u001b[39;00m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps)\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;66;03m# Setup the memory\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     memory \u001b[38;5;241m=\u001b[39m check_memory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory)\n",
      "File \u001b[1;32mc:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:258\u001b[0m, in \u001b[0;36mPipeline._validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[0;32m    256\u001b[0m         t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m     ):\n\u001b[1;32m--> 258\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    259\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll intermediate steps should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers and implement fit and transform \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor be the string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (t, \u001b[38;5;28mtype\u001b[39m(t))\n\u001b[0;32m    263\u001b[0m         )\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# We allow last estimator to be None as an identity transformation\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    267\u001b[0m     estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m ):\n",
      "\u001b[1;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<function convertir_minusculas at 0x000002662707F1A0>' (type <class 'function'>) doesn't"
     ]
    }
   ],
   "source": [
    "datos_limpios.to_csv(\"Datos_limpios.csv\", index=False)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"duplicados\", EliminarDuplicados([\"Review\"])),\n",
    "    (\"minusculas\", convertir_minusculas),\n",
    "    (\"numeros enteros\", convertir_enteros_a_texto),\n",
    "    (\"ascii\", remove_non_ascii),\n",
    "    (\"caracteres_especiales\", eliminar_caracteres_especiales),\n",
    "    (\"lematización\", lemmatize_spanish_text_batch),\n",
    "    (\"puntuacion\", eliminar_tildes_y_puntuacion),\n",
    "    (\"stopwords\", remove_stopwords),\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=3500)),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# Entrena el pipeline\n",
    "X_train, y_train = datos_limpios['Review'], datos_limpios['Class']\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "joblib.dump(pipeline, 'pipeline.joblib')\n",
    "loaded_pipeline = joblib.load('pipeline.joblib')\n",
    "\n",
    "data_2 = pd.read_csv(\"particion_prueba_estudiantes.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "datos_limpios_2 = data_2.copy()\n",
    "datos_limpios_2 = eliminar_duplicados(datos_limpios_2, [\"Review\"])\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(convertir_minusculas)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(convertir_enteros_a_texto)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(remove_non_ascii)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(eliminar_caracteres_especiales)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(eliminar_tildes_y_puntuacion)\n",
    "datos_limpios_2['Review'] = datos_limpios_2['Review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "64594b82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_pipeline\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(datos_limpios_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Informe de clasificación\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReporte de la Clasificación:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_pipeline.predict(datos_limpios_2['Review'])\n",
    "\n",
    "# Informe de clasificación\n",
    "print(\"Reporte de la Clasificación:\\n\")\n",
    "print(classification_report(['Class'], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
